{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ZXhd8FdUt2C"
      },
      "source": [
        "# Task 2: Graph Generation on QM9 Dataset with DDPM\n",
        "\n",
        "\n",
        "This notebook is adapted from the National University of Singapore CS5284 Graph Machine Learning course, the original tutorial code can be found at [here](https://github.com/xbresson/CS5284_2024/blob/main/codes/11_Graph_Generation/code06_solution.ipynb).\n",
        "\n",
        "> Some sections are adapted from the generation of GPT-4o and o3-mini models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gvca_z_VUt2J",
        "outputId": "e069bc04-d783-4d56-a757-9663e7a373ab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive/\n",
            "/content/drive/MyDrive/gt_v2\n",
            "Collecting dgl==1.0.0\n",
            "  Downloading dgl-1.0.0-cp311-cp311-manylinux1_x86_64.whl.metadata (530 bytes)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from dgl==1.0.0) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from dgl==1.0.0) (1.13.1)\n",
            "Requirement already satisfied: networkx>=2.1 in /usr/local/lib/python3.11/dist-packages (from dgl==1.0.0) (3.4.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from dgl==1.0.0) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from dgl==1.0.0) (4.67.1)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.11/dist-packages (from dgl==1.0.0) (5.9.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->dgl==1.0.0) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->dgl==1.0.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->dgl==1.0.0) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->dgl==1.0.0) (2024.12.14)\n",
            "Downloading dgl-1.0.0-cp311-cp311-manylinux1_x86_64.whl (5.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m40.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: dgl\n",
            "Successfully installed dgl-1.0.0\n",
            "Collecting rdkit==2023.09.6\n",
            "  Downloading rdkit-2023.9.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.9 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from rdkit==2023.09.6) (1.26.4)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from rdkit==2023.09.6) (11.1.0)\n",
            "Downloading rdkit-2023.9.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.9/34.9 MB\u001b[0m \u001b[31m41.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: rdkit\n",
            "Successfully installed rdkit-2023.9.6\n"
          ]
        }
      ],
      "source": [
        "# For Google Colaboratory\n",
        "import sys, os\n",
        "if \"google.colab\" in sys.modules:\n",
        "    from google.colab import drive\n",
        "    drive.mount(\"/content/drive/\")\n",
        "    path = \"/content/drive/MyDrive/yanming_dissertation/gtv2/code/\"\n",
        "    os.chdir(path)\n",
        "    !pwd\n",
        "    !pip install dgl==1.0.0\n",
        "    !pip install rdkit==2023.09.6"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i2lb5gT0Ut2L",
        "outputId": "64a6ac3c-91e9-4ca1-bd04-67c6079af6dc"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "DGL backend not selected or invalid.  Assuming PyTorch for now.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Setting the default backend to \"pytorch\". You can change it in the ~/.dgl/config.json file or export the DGLBACKEND environment variable.  Valid options are: pytorch, mxnet, tensorflow (all lowercase)\n"
          ]
        }
      ],
      "source": [
        "# Libraries\n",
        "import pickle\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn as nn\n",
        "import time\n",
        "import networkx as nx\n",
        "import sys; sys.path.insert(0, \"lib/\")\n",
        "from lib.molecules import Dictionary, MoleculeDataset, MoleculeDGL, Molecule, compute_ncut, from_pymol_to_smile\n",
        "import os, datetime\n",
        "from rdkit import Chem\n",
        "from rdkit.Chem import Descriptors\n",
        "from rdkit.Chem import rdmolops\n",
        "from rdkit import RDLogger\n",
        "RDLogger.DisableLog(\"rdApp.*\")\n",
        "import numpy as np\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_5TrPWZCUt2M",
        "outputId": "70ee8825-ae62-41da-b709-7db82a4a15b7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2.5.1+cu124\n",
            "Tesla T4\n",
            "cuda\n"
          ]
        }
      ],
      "source": [
        "# PyTorch version and GPU\n",
        "print(torch.__version__)\n",
        "if torch.cuda.is_available():\n",
        "  print(torch.cuda.get_device_name(0))\n",
        "  device= torch.device(\"cuda\")\n",
        "else:\n",
        "  device= torch.device(\"cpu\")\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tjqOuIUmUt2M"
      },
      "source": [
        "## Load Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 566
        },
        "id": "TMSsmWEfUt2N",
        "outputId": "db32a7a7-6187-4a65-f6f0-de72b1ad0d65"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading data...\n",
            "dataset/QM9/\n",
            "Time: 6.1628 sec\n",
            "num train data : 1000\n",
            "atom_dict.idx2word : ['N', 'C', 'O', 'F', 'N H3 +', 'O -', 'C H1 -', 'N +', 'N -']\n",
            "atom_dict.word2idx : {'N': 0, 'C': 1, 'O': 2, 'F': 3, 'N H3 +': 4, 'O -': 5, 'C H1 -': 6, 'N +': 7, 'N -': 8}\n",
            "bond_dict.idx2word : ['NONE', 'SINGLE', 'DOUBLE', 'TRIPLE']\n",
            "bond_dict.word2idx : {'NONE': 0, 'SINGLE': 1, 'DOUBLE': 2, 'TRIPLE': 3}\n",
            "9 4\n",
            "train[idx].atom_type : tensor([1, 1, 1, 2, 1, 1, 2, 1, 1])\n",
            "train[idx].atom_type_pe : tensor([0, 1, 2, 0, 3, 4, 1, 5, 6])\n",
            "train[idx].bond_type : tensor([[0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [1, 0, 1, 0, 0, 0, 0, 0, 1],\n",
            "        [0, 1, 0, 1, 1, 0, 0, 0, 0],\n",
            "        [0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
            "        [0, 0, 1, 0, 0, 1, 0, 0, 1],\n",
            "        [0, 0, 0, 0, 1, 0, 1, 1, 0],\n",
            "        [0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 0, 1, 0, 0, 1],\n",
            "        [0, 1, 0, 0, 1, 0, 0, 1, 0]])\n",
            "train[idx].bag_of_atoms : tensor([0, 7, 2, 0, 0, 0, 0, 0, 0])\n",
            "train[idx].smile:  CC1C(O)C2C(O)CC12\n",
            "train[idx].logP_SA tensor([-4.3320])\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<rdkit.Chem.rdchem.Mol at 0x7c5f2c5c8a50>"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "print(\"Loading data...\")\n",
        "start = time.time()\n",
        "\n",
        "data_folder_pytorch = \"dataset/QM9/\"\n",
        "print(data_folder_pytorch)\n",
        "\n",
        "with open(data_folder_pytorch+\"atom_dict.pkl\",\"rb\") as f:\n",
        "    atom_dict=pickle.load(f)\n",
        "with open(data_folder_pytorch+\"bond_dict.pkl\",\"rb\") as f:\n",
        "    bond_dict=pickle.load(f)\n",
        "with open(data_folder_pytorch+\"train.pkl\",\"rb\") as f:\n",
        "    train=pickle.load(f)\n",
        "with open(data_folder_pytorch+\"val.pkl\",\"rb\") as f:\n",
        "    val=pickle.load(f)\n",
        "with open(data_folder_pytorch+\"test.pkl\",\"rb\") as f:\n",
        "    test=pickle.load(f)\n",
        "\n",
        "print(f\"Time: {time.time() - start:.4f} sec\")\n",
        "\n",
        "print(\"num train data :\", len(train))\n",
        "print(\"num valid data :\", len(val))\n",
        "print(\"num test data :\", len(test))\n",
        "\n",
        "print(\"atom_dict.idx2word :\", atom_dict.idx2word)\n",
        "print(\"atom_dict.word2idx :\", atom_dict.word2idx)\n",
        "print(\"bond_dict.idx2word :\", bond_dict.idx2word)\n",
        "print(\"bond_dict.word2idx :\", bond_dict.word2idx)\n",
        "\n",
        "num_atom_type = len(atom_dict.idx2word)\n",
        "num_bond_type = len(bond_dict.idx2word)\n",
        "print(num_atom_type, num_bond_type)\n",
        "\n",
        "idx = 0\n",
        "print(\"train[idx].atom_type :\", train[idx].atom_type)\n",
        "print(\"train[idx].atom_type_pe :\", train[idx].atom_type_pe)\n",
        "print(\"train[idx].bond_type :\", train[idx].bond_type)\n",
        "print(\"train[idx].bag_of_atoms :\", train[idx].bag_of_atoms)\n",
        "print(\"train[idx].smile: \", train[idx].smile)\n",
        "print(\"train[idx].logP_SA\", train[idx].logP_SA)\n",
        "mol = Chem.MolFromSmiles(train[idx].smile)\n",
        "mol"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xahWlDsLUt2N"
      },
      "source": [
        "## Dataset Statistics\n",
        "\n",
        "### Grouping Molecules by Number of Atoms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MP8msgBsUt2O",
        "outputId": "51a3a769-fcb5-47a9-fb04-3dc87c0db9fd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Max num atoms =  9\n",
            "\n",
            "Train\n",
            "number of molecule of size 4: \t 1\n",
            "number of molecule of size 5: \t 1\n",
            "number of molecule of size 6: \t 7\n",
            "number of molecule of size 7: \t 24\n",
            "number of molecule of size 8: \t 136\n",
            "number of molecule of size 9: \t 831\n",
            "Val\n",
            "number of molecule of size 7: \t 6\n",
            "number of molecule of size 8: \t 28\n",
            "number of molecule of size 9: \t 166\n",
            "Test\n",
            "number of molecule of size 6: \t 1\n",
            "number of molecule of size 7: \t 3\n",
            "number of molecule of size 8: \t 37\n",
            "number of molecule of size 9: \t 159\n"
          ]
        }
      ],
      "source": [
        "# Organize data into group of of molecules of fixed sized\n",
        "# Example: train[9] is a list containing all the molecules of size 9\n",
        "def group_molecules_per_size(dataset):\n",
        "    mydict = {}\n",
        "    for mol in dataset:\n",
        "        if len(mol) not in mydict:\n",
        "            mydict[len(mol)] = []\n",
        "        mydict[len(mol)].append(mol)\n",
        "    return mydict\n",
        "\n",
        "test_group  = group_molecules_per_size(test)\n",
        "val_group   = group_molecules_per_size(val)\n",
        "train_group = group_molecules_per_size(train)\n",
        "\n",
        "# The biggest molecule in the train set\n",
        "max_mol_sz= max(list( train_group.keys()))\n",
        "print(\"Max num atoms = \", max_mol_sz)\n",
        "\n",
        "# Print distribution w.r.t. molecule size\n",
        "def print_distribution(data):\n",
        "    for nb_atom in range(max_mol_sz+1):\n",
        "        try:\n",
        "            print(\"number of molecule of size {}: \\t {}\".format(nb_atom, len(data[nb_atom])))\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "print()\n",
        "print(\"Train\"); print_distribution(train_group)\n",
        "print(\"Val\"); print_distribution(val_group)\n",
        "print(\"Test\"); print_distribution(test_group)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "plT8zymcTTzz"
      },
      "source": [
        "### Sampling Molecules from Train Set through Possibility Distribution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vhfcQd-qTTzz",
        "outputId": "26ae8545-775e-4692-b726-e3090d5db7cd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "sampler_size.num_mol : {9: 831, 8: 136, 7: 24, 6: 7, 5: 1, 4: 1}\n",
            "sz : 8\n"
          ]
        }
      ],
      "source": [
        "class sample_molecule_size:\n",
        "    def __init__(self, organized_dataset):\n",
        "        self.num_mol = {sz: len(list_of_mol) for sz, list_of_mol in organized_dataset.items()}\n",
        "        self.num_batches_remaining = {sz: self.num_mol[sz] for sz in self.num_mol}\n",
        "    def choose_molecule_size(self):\n",
        "        num_batches = self.num_batches_remaining\n",
        "        possible_sizes = np.array( list( num_batches.keys()) )\n",
        "        prob = np.array( list( num_batches.values() )   )\n",
        "        prob = prob / prob.sum()\n",
        "        sz = np.random.choice(  possible_sizes , p=prob )\n",
        "        return sz\n",
        "\n",
        "sampler_size = sample_molecule_size(train_group)\n",
        "print('sampler_size.num_mol :',sampler_size.num_mol)\n",
        "sz = sampler_size.choose_molecule_size()\n",
        "print('sz :',sz)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Ynx7hRWUt2O"
      },
      "source": [
        "## Generate Batches\n",
        "\n",
        "### Implement the molecule sampler class for batch sampling of molecules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "mr3fGKBeUt2P"
      },
      "outputs": [],
      "source": [
        "class MoleculeSampler:\n",
        "    def __init__(self, organized_dataset, bs, shuffle=True):\n",
        "        self.bs = bs\n",
        "        self.num_mol =  {sz: len(list_of_mol) for sz, list_of_mol in organized_dataset.items()}\n",
        "        self.counter = {sz: 0   for sz in organized_dataset}\n",
        "        if shuffle:\n",
        "            self.order = {sz: np.random.permutation(num)  for sz , num in self.num_mol.items()}\n",
        "        else:\n",
        "            self.order = {sz: np.arange(num)  for sz , num in self.num_mol.items()}\n",
        "\n",
        "    def compute_num_batches_remaining(self):\n",
        "        return {sz:  math.ceil(((self.num_mol[sz] - self.counter[sz])/self.bs))  for sz in self.num_mol}\n",
        "\n",
        "    def choose_molecule_size(self):\n",
        "        num_batches = self.compute_num_batches_remaining()\n",
        "        possible_sizes = np.array(list(num_batches.keys()))\n",
        "        prob = np.array(list(num_batches.values()))\n",
        "        prob = prob / prob.sum()\n",
        "        sz   = np.random.choice(possible_sizes, p=prob)\n",
        "        return sz\n",
        "\n",
        "    def is_empty(self):\n",
        "        num_batches= self.compute_num_batches_remaining()\n",
        "        return sum(num_batches.values()) == 0\n",
        "\n",
        "    def draw_batch_of_molecules(self, sz):\n",
        "        if (self.num_mol[sz] - self.counter[sz]) / self.bs >= 1.0:\n",
        "            bs = self.bs\n",
        "        else:\n",
        "            bs = self.num_mol[sz] - (self.num_mol[sz] // self.bs) * self.bs\n",
        "        indices = self.order[sz][self.counter[sz]:self.counter[sz] + bs]\n",
        "        self.counter[sz] += bs\n",
        "        return indices"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VPFhgO6UUt2P"
      },
      "source": [
        "### Extract one example mini-batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lIs10DlGUt2P",
        "outputId": "d9530123-e0a4-45a7-d6b8-5070c026250b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "sampler.num_mol : {9: 831, 8: 136, 7: 24, 6: 7, 5: 1, 4: 1}\n",
            "num_batches_remaining : {9: 17, 8: 3, 7: 1, 6: 1, 5: 1, 4: 1}\n",
            "sz : 5\n",
            "indices : 1 [0]\n",
            "minibatch_node : torch.Size([1, 5])\n",
            "minibatch_pe : torch.Size([1, 5])\n",
            "minibatch_edge : torch.Size([1, 5, 5])\n",
            "minibatch_boa : torch.Size([1, 9])\n"
          ]
        }
      ],
      "source": [
        "# extract one mini-batch\n",
        "bs = 50\n",
        "sampler = MoleculeSampler(train_group, bs)\n",
        "print('sampler.num_mol :', sampler.num_mol)\n",
        "\n",
        "num_batches_remaining = sampler.compute_num_batches_remaining()\n",
        "print('num_batches_remaining :', num_batches_remaining)\n",
        "sz = sampler.choose_molecule_size()\n",
        "print('sz :', sz)\n",
        "indices = sampler.draw_batch_of_molecules(sz)\n",
        "print('indices :', len(indices), indices)\n",
        "minibatch_node = torch.stack([train_group[sz][i].atom_type for i in indices])\n",
        "print('minibatch_node :', minibatch_node.size())\n",
        "minibatch_pe  = torch.stack([train_group[sz][i].atom_type_pe for i in indices])\n",
        "print('minibatch_pe :', minibatch_pe.size())\n",
        "minibatch_edge = torch.stack([ train_group[sz][i].bond_type for i in indices])\n",
        "print('minibatch_edge :', minibatch_edge.size())\n",
        "minibatch_boa = torch.stack([train_group[sz][i].bag_of_atoms for i in indices])\n",
        "print('minibatch_boa :', minibatch_boa.size())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WZunx_C9Ut2Q"
      },
      "source": [
        "## DDPM Model\n",
        "\n",
        "### Global Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Iy7fOuGUt2Q",
        "outputId": "49bca263-3e3a-4cc2-c98d-5c691294585e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "num_heads, d, num_layers, dPEt, drop, bs :  4 128 6 128 0.0 10\n",
            "beta_1, beta_T, num_t : 0.0001 0.02 150\n",
            "num_warmup : 200\n"
          ]
        }
      ],
      "source": [
        "# Global constants\n",
        "num_heads = 4 # number of heads in the transformer layer\n",
        "d = 32 * num_heads # number of hidden dimensions\n",
        "num_layers = 6 # number of transformer layers\n",
        "dPEt = d # number of dimensions for the time step of the diffusion model\n",
        "drop = 0.0 # dropout value\n",
        "bs = 10 # batch size\n",
        "\n",
        "print('num_heads, d, num_layers, dPEt, drop, bs : ', num_heads, d, num_layers, dPEt, drop, bs)\n",
        "\n",
        "beta_1 = 0.0001 # beta_1 for DM\n",
        "beta_T = 0.02 # beta_T for DM\n",
        "num_t = 150 # number of time steps of the DM\n",
        "alpha_t = 1.0 - torch.linspace(beta_1, beta_T, num_t).to(device) # for DM, size=[num_t]\n",
        "alpha_bar_t = torch.cumprod( alpha_t, dim=0) # for DM, size=[num_t]\n",
        "print('beta_1, beta_T, num_t :', beta_1, beta_T, num_t)\n",
        "\n",
        "# Warmup\n",
        "num_mol_size = 20\n",
        "num_warmup = 2 * max( num_mol_size, len(train) // bs ) # 4 epochs * max( num_mol_size=20, num_mol/batch_size)\n",
        "print('num_warmup :',num_warmup)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dlpmO2lTUt2S"
      },
      "source": [
        "### Instantiate and test the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WOUDjIg9Ut2S",
        "outputId": "91e5b36e-1b5a-4ef8-c8b3-250d14da4331"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of parameters: 2767629 (2.77 million)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "sampler.num_mol : {9: 831, 8: 136, 7: 24, 6: 7, 5: 1, 4: 1}\n",
            "num_batches_remaining : {9: 84, 8: 14, 7: 3, 6: 1, 5: 1, 4: 1}\n",
            "sz : 5\n",
            "minibatch_node : torch.Size([1, 5])\n",
            "minibatch_edge : torch.Size([1, 5, 5])\n",
            "batch_sample_t torch.Size([1])\n",
            "batch_noise_x_t torch.Size([1, 5, 9]) batch_noise_e_t torch.Size([1, 5, 5, 4])\n",
            "x_t torch.Size([1, 5, 9]) e_t torch.Size([1, 5, 5, 4])\n",
            "noise_pred_x_t torch.Size([1, 5, 9]) noise_pred_e_t torch.Size([1, 5, 5, 4])\n",
            "batch_x_0 torch.Size([4, 9, 9]) batch_e_0 torch.Size([4, 9, 9, 4])\n"
          ]
        }
      ],
      "source": [
        "from models.gtv1 import UNet_vanilla\n",
        "from models.gtv2_weighted import UNet_weighted\n",
        "from models.gtv2_gated import UNet_gated\n",
        "from models.gtv2_mixed import UNet_mixed\n",
        "from models.gtv2_film import UNet_film\n",
        "from models.ddpm import DDPM\n",
        "\n",
        "def sym_tensor(x):\n",
        "    x = x.permute(0,3,1,2) # [bs, d, n, n]\n",
        "    triu = torch.triu(x,diagonal=1).transpose(3,2) # [bs, d, n, n]\n",
        "    mask = (triu.abs()>0).float()                  # [bs, d, n, n]\n",
        "    x =  x * (1 - mask ) + mask * triu             # [bs, d, n, n]\n",
        "    x = x.permute(0,2,3,1) # [bs, n, n, d]\n",
        "    return x               # [bs, n, n, d]\n",
        "\n",
        "# Instantiate the network\n",
        "gt = UNet_film(d, num_heads, num_layers, num_atom_type, num_bond_type, num_t, max_mol_sz, dPEt, device, drop)\n",
        "net = DDPM(num_t, beta_1, beta_T, gt, num_atom_type, num_bond_type, device).to(device)\n",
        "\n",
        "def display_num_param(net):\n",
        "    nb_param = 0\n",
        "    for param in net.parameters():\n",
        "        nb_param += param.numel()\n",
        "    print('Number of parameters: {} ({:.2f} million)'.format(nb_param, nb_param/1e6))\n",
        "    return nb_param/1e6\n",
        "_ = display_num_param(net)\n",
        "\n",
        "# Test the forward pass, backward pass and gradient update with a single batch\n",
        "init_lr = 0.001\n",
        "optimizer = torch.optim.AdamW(net.parameters(), lr=init_lr)\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.95, patience=1, verbose=True)\n",
        "\n",
        "sampler = MoleculeSampler(train_group, bs)\n",
        "print('sampler.num_mol :',sampler.num_mol)\n",
        "num_batches_remaining = sampler.compute_num_batches_remaining()\n",
        "print('num_batches_remaining :',num_batches_remaining)\n",
        "sz = sampler.choose_molecule_size()\n",
        "print('sz :',sz)\n",
        "indices = sampler.draw_batch_of_molecules(sz)\n",
        "batch_x0 = minibatch_node = torch.stack( [ train_group[sz][i].atom_type for i in indices] ).long().to(device) # [bs, n]\n",
        "print('minibatch_node :',minibatch_node.size())\n",
        "batch_e0 = minibatch_edge = torch.stack( [ train_group[sz][i].bond_type for i in indices] ).long().to(device) # [bs, n, n]\n",
        "print('minibatch_edge :',minibatch_edge.size())\n",
        "batch_sample_t = torch.randint(0, num_t, (batch_x0.size(0),)).long().to(device) # random interger in {0,1,...,T-1} [bs]\n",
        "print('batch_sample_t',batch_sample_t.size())\n",
        "\n",
        "bs2, n = batch_x0.size()\n",
        "batch_noise_x_t = torch.randn(bs2,n,num_atom_type).to(device) # [bs, n, num_atom_type]\n",
        "batch_noise_e_t = torch.randn(bs2,n,n,num_bond_type).to(device) # [bs, n, n, num_bond_type]\n",
        "batch_noise_e_t = sym_tensor(batch_noise_e_t)\n",
        "\n",
        "print('batch_noise_x_t',batch_noise_x_t.size(),'batch_noise_e_t',batch_noise_e_t.size())\n",
        "x_t, e_t = net.forward_process(batch_x0, batch_e0, batch_sample_t, batch_noise_x_t, batch_noise_e_t) # [bs, n], [bs, n, n]\n",
        "print('x_t',x_t.size(), 'e_t',e_t.size())\n",
        "\n",
        "noise_pred_x_t, noise_pred_e_t = net.backward_process(x_t, e_t, batch_sample_t) # [bs, n], [bs, n, n]\n",
        "print('noise_pred_x_t',noise_pred_x_t.size(),'noise_pred_e_t',noise_pred_e_t.size())\n",
        "\n",
        "loss_DDPM = torch.nn.MSELoss()(noise_pred_x_t, batch_noise_x_t) + 1.0* torch.nn.MSELoss()(noise_pred_e_t, batch_noise_e_t)\n",
        "loss = loss_DDPM\n",
        "optimizer.zero_grad()\n",
        "loss.backward()\n",
        "torch.nn.utils.clip_grad_norm_(net.parameters(), 0.25) # grad_norm_clip=1.0\n",
        "optimizer.step()\n",
        "\n",
        "with torch.no_grad():\n",
        "    batch_x_0, batch_e_0 = net.generate_process_ddpm(4, 9)\n",
        "    print('batch_x_0',batch_x_0.size(), 'batch_e_0',batch_e_0.size())\n",
        "\n",
        "del gt, net\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yVA6tjsjUt2T"
      },
      "source": [
        "## Train And Evaluate The Model\n",
        "\n",
        "### Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AKhmIjB1Ut2T",
        "outputId": "43b4f301-2dcf-44f0-af63-1ccf7477bd36"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "num_heads, d, num_layers, dPEt, drop, bs :  4 128 6 128 0.0 10\n",
            "beta_1, beta_T, num_t : 0.0001 0.02 150\n",
            "num_warmup, nb_epochs : 200 100\n",
            "epoch= 0 \t time= 0.2392 min \t lr= 0.0001575 \t loss= 2.0997\n",
            "epoch= 1 \t time= 0.4685 min \t lr= 0.0003000 \t loss= 0.8702\n",
            "epoch= 2 \t time= 0.7043 min \t lr= 0.0003000 \t loss= 0.5315\n",
            "epoch= 3 \t time= 0.9366 min \t lr= 0.0003000 \t loss= 0.4405\n",
            "epoch= 4 \t time= 1.1881 min \t lr= 0.0003000 \t loss= 0.3861\n",
            "epoch= 5 \t time= 1.4277 min \t lr= 0.0003000 \t loss= 0.3588\n",
            "epoch= 6 \t time= 1.6611 min \t lr= 0.0003000 \t loss= 0.3345\n",
            "epoch= 7 \t time= 1.8925 min \t lr= 0.0003000 \t loss= 0.3189\n",
            "epoch= 8 \t time= 2.1258 min \t lr= 0.0003000 \t loss= 0.2702\n",
            "epoch= 9 \t time= 2.3538 min \t lr= 0.0003000 \t loss= 0.2557\n",
            "epoch= 10 \t time= 2.5890 min \t lr= 0.0003000 \t loss= 0.2628\n",
            "epoch= 11 \t time= 2.8211 min \t lr= 0.0003000 \t loss= 0.2401\n",
            "epoch= 12 \t time= 3.0495 min \t lr= 0.0003000 \t loss= 0.2071\n",
            "epoch= 13 \t time= 3.2811 min \t lr= 0.0003000 \t loss= 0.2074\n",
            "epoch= 14 \t time= 3.5084 min \t lr= 0.0003000 \t loss= 0.1962\n",
            "epoch= 15 \t time= 3.7412 min \t lr= 0.0003000 \t loss= 0.1882\n",
            "epoch= 16 \t time= 3.9718 min \t lr= 0.0003000 \t loss= 0.1718\n",
            "epoch= 17 \t time= 4.2036 min \t lr= 0.0003000 \t loss= 0.1664\n",
            "epoch= 18 \t time= 4.4351 min \t lr= 0.0003000 \t loss= 0.1634\n",
            "epoch= 19 \t time= 4.6639 min \t lr= 0.0003000 \t loss= 0.1758\n",
            "epoch= 20 \t time= 4.8956 min \t lr= 0.0003000 \t loss= 0.1580\n",
            "epoch= 21 \t time= 5.1238 min \t lr= 0.0003000 \t loss= 0.1647\n",
            "epoch= 22 \t time= 5.3538 min \t lr= 0.0003000 \t loss= 0.1526\n",
            "epoch= 23 \t time= 5.5886 min \t lr= 0.0003000 \t loss= 0.1537\n",
            "epoch= 24 \t time= 5.8170 min \t lr= 0.0002850 \t loss= 0.1560\n",
            "epoch= 25 \t time= 6.0489 min \t lr= 0.0002850 \t loss= 0.1392\n",
            "epoch= 26 \t time= 6.2784 min \t lr= 0.0002850 \t loss= 0.1322\n",
            "epoch= 27 \t time= 6.5078 min \t lr= 0.0002850 \t loss= 0.1388\n",
            "epoch= 28 \t time= 6.7398 min \t lr= 0.0002707 \t loss= 0.1334\n",
            "epoch= 29 \t time= 6.9712 min \t lr= 0.0002707 \t loss= 0.1447\n",
            "epoch= 30 \t time= 7.2006 min \t lr= 0.0002707 \t loss= 0.1283\n",
            "epoch= 31 \t time= 7.4315 min \t lr= 0.0002707 \t loss= 0.1543\n",
            "epoch= 32 \t time= 7.6595 min \t lr= 0.0002572 \t loss= 0.1316\n",
            "epoch= 33 \t time= 7.8907 min \t lr= 0.0002572 \t loss= 0.1201\n",
            "epoch= 34 \t time= 8.1209 min \t lr= 0.0002572 \t loss= 0.1239\n",
            "epoch= 35 \t time= 8.3523 min \t lr= 0.0002444 \t loss= 0.1336\n",
            "epoch= 36 \t time= 8.5855 min \t lr= 0.0002444 \t loss= 0.1211\n",
            "epoch= 37 \t time= 8.8175 min \t lr= 0.0002321 \t loss= 0.1201\n",
            "epoch= 38 \t time= 9.0474 min \t lr= 0.0002321 \t loss= 0.1050\n",
            "epoch= 39 \t time= 9.2835 min \t lr= 0.0002321 \t loss= 0.1134\n",
            "epoch= 40 \t time= 9.5166 min \t lr= 0.0002205 \t loss= 0.1159\n",
            "epoch= 41 \t time= 9.7475 min \t lr= 0.0002205 \t loss= 0.1159\n",
            "epoch= 42 \t time= 9.9818 min \t lr= 0.0002095 \t loss= 0.1137\n",
            "epoch= 43 \t time= 10.2141 min \t lr= 0.0002095 \t loss= 0.1101\n",
            "epoch= 44 \t time= 10.4469 min \t lr= 0.0002095 \t loss= 0.1050\n",
            "epoch= 45 \t time= 10.6819 min \t lr= 0.0002095 \t loss= 0.1018\n",
            "epoch= 46 \t time= 10.9154 min \t lr= 0.0002095 \t loss= 0.1062\n",
            "epoch= 47 \t time= 11.1453 min \t lr= 0.0001990 \t loss= 0.1025\n",
            "epoch= 48 \t time= 11.3786 min \t lr= 0.0001990 \t loss= 0.1028\n",
            "epoch= 49 \t time= 11.6090 min \t lr= 0.0001990 \t loss= 0.0998\n",
            "epoch= 50 \t time= 11.8405 min \t lr= 0.0001990 \t loss= 0.0987\n",
            "epoch= 51 \t time= 12.0715 min \t lr= 0.0001990 \t loss= 0.1003\n",
            "epoch= 52 \t time= 12.3039 min \t lr= 0.0001990 \t loss= 0.0919\n",
            "epoch= 53 \t time= 12.5309 min \t lr= 0.0001990 \t loss= 0.0975\n",
            "epoch= 54 \t time= 12.7646 min \t lr= 0.0001891 \t loss= 0.0993\n",
            "epoch= 55 \t time= 12.9953 min \t lr= 0.0001891 \t loss= 0.0919\n",
            "epoch= 56 \t time= 13.2225 min \t lr= 0.0001891 \t loss= 0.1019\n",
            "epoch= 57 \t time= 13.4527 min \t lr= 0.0001796 \t loss= 0.0974\n",
            "epoch= 58 \t time= 13.6802 min \t lr= 0.0001796 \t loss= 0.1044\n",
            "epoch= 59 \t time= 13.9112 min \t lr= 0.0001706 \t loss= 0.1005\n",
            "epoch= 60 \t time= 14.1402 min \t lr= 0.0001706 \t loss= 0.0972\n",
            "epoch= 61 \t time= 14.3703 min \t lr= 0.0001621 \t loss= 0.0957\n",
            "epoch= 62 \t time= 14.6002 min \t lr= 0.0001621 \t loss= 0.0946\n",
            "epoch= 63 \t time= 14.8305 min \t lr= 0.0001621 \t loss= 0.0868\n",
            "epoch= 64 \t time= 15.0635 min \t lr= 0.0001621 \t loss= 0.0898\n",
            "epoch= 65 \t time= 15.2942 min \t lr= 0.0001621 \t loss= 0.0845\n",
            "epoch= 66 \t time= 15.5280 min \t lr= 0.0001621 \t loss= 0.0880\n",
            "epoch= 67 \t time= 15.7575 min \t lr= 0.0001540 \t loss= 0.0875\n",
            "epoch= 68 \t time= 15.9893 min \t lr= 0.0001540 \t loss= 0.0880\n",
            "epoch= 69 \t time= 16.2201 min \t lr= 0.0001540 \t loss= 0.0835\n",
            "epoch= 70 \t time= 16.4525 min \t lr= 0.0001540 \t loss= 0.0901\n",
            "epoch= 71 \t time= 16.6802 min \t lr= 0.0001463 \t loss= 0.0917\n",
            "epoch= 72 \t time= 16.9125 min \t lr= 0.0001463 \t loss= 0.0864\n",
            "epoch= 73 \t time= 17.1427 min \t lr= 0.0001390 \t loss= 0.0900\n",
            "epoch= 74 \t time= 17.3708 min \t lr= 0.0001390 \t loss= 0.0852\n",
            "epoch= 75 \t time= 17.5990 min \t lr= 0.0001390 \t loss= 0.0812\n",
            "epoch= 76 \t time= 17.8317 min \t lr= 0.0001390 \t loss= 0.0840\n",
            "epoch= 77 \t time= 18.0602 min \t lr= 0.0001390 \t loss= 0.0803\n",
            "epoch= 78 \t time= 18.2935 min \t lr= 0.0001390 \t loss= 0.0770\n",
            "epoch= 79 \t time= 18.5217 min \t lr= 0.0001390 \t loss= 0.0805\n",
            "epoch= 80 \t time= 18.7500 min \t lr= 0.0001320 \t loss= 0.0871\n",
            "epoch= 81 \t time= 18.9801 min \t lr= 0.0001320 \t loss= 0.0814\n",
            "epoch= 82 \t time= 19.2076 min \t lr= 0.0001254 \t loss= 0.0797\n",
            "epoch= 83 \t time= 19.4368 min \t lr= 0.0001254 \t loss= 0.0815\n",
            "epoch= 84 \t time= 19.6670 min \t lr= 0.0001192 \t loss= 0.0791\n",
            "epoch= 85 \t time= 19.8975 min \t lr= 0.0001192 \t loss= 0.0772\n",
            "epoch= 86 \t time= 20.1278 min \t lr= 0.0001132 \t loss= 0.0791\n",
            "epoch= 87 \t time= 20.3563 min \t lr= 0.0001132 \t loss= 0.0866\n",
            "epoch= 88 \t time= 20.5888 min \t lr= 0.0001075 \t loss= 0.0808\n",
            "epoch= 89 \t time= 20.8206 min \t lr= 0.0001075 \t loss= 0.0769\n",
            "epoch= 90 \t time= 21.0506 min \t lr= 0.0001075 \t loss= 0.0851\n",
            "epoch= 91 \t time= 21.2868 min \t lr= 0.0001075 \t loss= 0.0768\n",
            "epoch= 92 \t time= 21.5215 min \t lr= 0.0001075 \t loss= 0.0757\n",
            "epoch= 93 \t time= 21.7522 min \t lr= 0.0001075 \t loss= 0.0804\n",
            "epoch= 94 \t time= 21.9863 min \t lr= 0.0001075 \t loss= 0.0733\n",
            "epoch= 95 \t time= 22.2169 min \t lr= 0.0001075 \t loss= 0.0776\n",
            "epoch= 96 \t time= 22.4488 min \t lr= 0.0001022 \t loss= 0.0752\n",
            "epoch= 97 \t time= 22.6828 min \t lr= 0.0001022 \t loss= 0.0741\n",
            "epoch= 98 \t time= 22.9189 min \t lr= 0.0000971 \t loss= 0.0801\n",
            "epoch= 99 \t time= 23.1486 min \t lr= 0.0000971 \t loss= 0.0798\n"
          ]
        }
      ],
      "source": [
        "# Random seed\n",
        "seed = 0\n",
        "torch.manual_seed(seed)\n",
        "np.random.seed(seed)\n",
        "\n",
        "gt = UNet_film(d, num_heads, num_layers, num_atom_type, num_bond_type, num_t, max_mol_sz, dPEt, device, drop)\n",
        "net = DDPM(num_t, beta_1, beta_T, gt, num_atom_type, num_bond_type, device).to(device)\n",
        "\n",
        "# Optimizer\n",
        "init_lr = 0.0003\n",
        "optimizer = torch.optim.AdamW(net.parameters(), lr=init_lr)\n",
        "scheduler_warmup = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda t: min((t+1)/num_warmup, 1.0) ) # warmup scheduler\n",
        "scheduler_tracker = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.95, patience=1, verbose=True) # tracker scheduler\n",
        "\n",
        "# Number of mini-batches per epoch\n",
        "nb_epochs = 100\n",
        "num_warmup_batch = 0\n",
        "print('num_heads, d, num_layers, dPEt, drop, bs : ', num_heads, d, num_layers, dPEt, drop, bs)\n",
        "print('beta_1, beta_T, num_t :', beta_1, beta_T, num_t)\n",
        "print('num_warmup, nb_epochs :',num_warmup, nb_epochs)\n",
        "\n",
        "start = time.time()\n",
        "for epoch in range(nb_epochs):\n",
        "    running_loss = 0.0\n",
        "    num_batches = 0\n",
        "    net.train()\n",
        "    sampler = MoleculeSampler(train_group, bs)\n",
        "    count = 0\n",
        "    while(not sampler.is_empty()):\n",
        "        sz = sampler.choose_molecule_size()\n",
        "        indices = sampler.draw_batch_of_molecules(sz)\n",
        "        batch_x0 = torch.stack( [ train_group[sz][i].atom_type for i in indices] ).long().to(device) # [bs, n]\n",
        "        batch_e0 = torch.stack( [ train_group[sz][i].bond_type for i in indices] ).long().to(device) # [bs, n, n]\n",
        "        batch_sample_t = torch.randint(0, num_t, (batch_x0.size(0),)).long().to(device) # random interger in {0,1,...,T-1} [bs]\n",
        "        bs2, n = batch_x0.size()\n",
        "        batch_noise_x_t = torch.randn(bs2,n,num_atom_type).to(device) # [bs, n, num_atom_type]\n",
        "        batch_noise_e_t = torch.randn(bs2,n,n,num_bond_type).to(device) # [bs, n, n, num_bond_type]\n",
        "        batch_noise_e_t = sym_tensor(batch_noise_e_t)\n",
        "        x_t, e_t = net.forward_process(batch_x0, batch_e0, batch_sample_t, batch_noise_x_t, batch_noise_e_t) # [bs, n], [bs, n, n]\n",
        "        noise_pred_x_t, noise_pred_e_t = net.backward_process(x_t, e_t, batch_sample_t) # [bs, n], [bs, n, n]\n",
        "        loss_DDPM = torch.nn.MSELoss()(noise_pred_x_t, batch_noise_x_t) + 1.0* torch.nn.MSELoss()(noise_pred_e_t, batch_noise_e_t)\n",
        "        loss = loss_DDPM\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(net.parameters(), 0.25) # grad_norm_clip=1.0\n",
        "        optimizer.step()\n",
        "        if num_warmup_batch < num_warmup:\n",
        "            scheduler_warmup.step() # warmup scheduler\n",
        "        num_warmup_batch += 1\n",
        "        # Compute stats\n",
        "        running_loss += loss.detach().item()\n",
        "        num_batches += 1\n",
        "    # Average stats\n",
        "    mean_loss = running_loss/ num_batches\n",
        "    if num_warmup_batch >= num_warmup:\n",
        "        scheduler_tracker.step(mean_loss) # tracker scheduler defined w.r.t. loss value\n",
        "    elapsed = (time.time()-start)/60\n",
        "    print('epoch= %d \\t time= %.4f min \\t lr= %.7f \\t loss= %.4f' % (epoch, elapsed, optimizer.param_groups[0]['lr'], mean_loss) )\n",
        "    # Check lr value\n",
        "    if optimizer.param_groups[0]['lr'] < 10**-6:\n",
        "      print(\"\\n lr is equal to min lr -- training stopped\\n\")\n",
        "      break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1B2I49_iTTz5"
      },
      "source": [
        "### Validity of Generated Molecules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mfB_ABMhTTz5",
        "outputId": "1f893878-c4ce-4310-cd6d-e59456e93188"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "num_atom_sampled : 9\n",
            "num_atom_sampled : 9\n",
            "num_atom_sampled : 9\n",
            "num_atom_sampled : 9\n",
            "num_atom_sampled : 9\n",
            "num_atom_sampled : 9\n",
            "num_atom_sampled : 9\n",
            "num_atom_sampled : 8\n",
            "num_atom_sampled : 8\n",
            "num_atom_sampled : 9\n",
            "percentage of valid molecules\n",
            "num_gen_mol= 1000   time(min)= 0.845   perc valid molecules= 48.1\n"
          ]
        }
      ],
      "source": [
        "# compute percentage of valid molecules\n",
        "def compute_perc_valid_molecules(net, sampler_size, num_gen_mol=1000, num_generated_mols_per_batch=100):\n",
        "    num_batches = num_gen_mol // num_generated_mols_per_batch\n",
        "    num_valid_mol = 0\n",
        "    list_valid_mol = []\n",
        "    list_mol = []\n",
        "    start = time.time()\n",
        "    for idx in range(num_batches):\n",
        "        net.eval()\n",
        "        net.UNet.eval()\n",
        "        with torch.no_grad():\n",
        "            num_atom_sampled = sampler_size.choose_molecule_size() # sample the molecule size\n",
        "            print('num_atom_sampled :',num_atom_sampled)\n",
        "            batch_x_0, batch_e_0 = net.generate_process_ddpm(num_generated_mols_per_batch, num_atom_sampled)\n",
        "            batch_x_0 = torch.max(batch_x_0, dim=2)[1]  # [bs, n]\n",
        "            batch_e_0 = torch.max(batch_e_0, dim=3)[1] # [bs, n, n]\n",
        "            x_hat = batch_x_0.detach().to('cpu')\n",
        "            e_hat = batch_e_0.detach().to('cpu')\n",
        "            for x,e in zip(x_hat,e_hat):\n",
        "                pymol = Molecule(num_atom_sampled, num_atom_type)\n",
        "                pymol.atom_type = x\n",
        "                pymol.bond_type = e\n",
        "                smile = from_pymol_to_smile(pymol, atom_dict, bond_dict)\n",
        "                list_mol.append(smile)\n",
        "                mol = Chem.MolFromSmiles(smile)\n",
        "                if mol is not None:\n",
        "                    list_valid_mol.append(smile)\n",
        "                    num_valid_mol += 1\n",
        "    perc_valid_molecules = 100*num_valid_mol/num_gen_mol\n",
        "    line = 'num_gen_mol= ' + str(num_gen_mol) + '   time(min)= ' + str((time.time()-start)/60)[:5] + '   perc valid molecules= ' + str(perc_valid_molecules)[:6]\n",
        "    return perc_valid_molecules, list_mol, line, list_valid_mol\n",
        "\n",
        "np.random.seed(0)\n",
        "perc_valid_mol, list_mol, line, list_valid_mol = compute_perc_valid_molecules(net, sampler_size)\n",
        "del gt, net\n",
        "torch.cuda.empty_cache()\n",
        "print('percentage of valid molecules')\n",
        "print(line)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jRfpKdQpTTz5"
      },
      "source": [
        "### Uniqueness of Generated Molecules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F3wb-qX7TTz6",
        "outputId": "092dcf69-90e1-4c5a-9038-29b44f87f154"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "num_generated_mol 1000\n",
            "num_unique_mol, num_mol: 1000 1000\n",
            "perc unique molecules among the generated molecules: 100.0\n"
          ]
        }
      ],
      "source": [
        "print('num_generated_mol',len(list_mol))\n",
        "num_unique_mol = 0\n",
        "for idx,mol in enumerate(list_mol):\n",
        "    list_tmp = list_mol.copy()\n",
        "    list_tmp.pop(idx)\n",
        "    if mol not in list_tmp:\n",
        "        num_unique_mol += 1\n",
        "print('num_unique_mol, num_mol:',num_unique_mol, len(list_mol))\n",
        "perc_unique_mol = 100*num_unique_mol/len(list_mol)\n",
        "print('perc unique molecules among the generated molecules:', str(perc_unique_mol)[:6])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ybEWEczUt2T"
      },
      "source": [
        "## Performance Comparison"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CoRPCRfyUt2T"
      },
      "source": [
        "Training was conducted over 100 epochs on a small subset of the QM9 dataset (1,000 training samples and 200 testing samples) with a fixed random seed. The results presented are the mean loss (and standard deviation) from the last 10 epochs, the percentage of valid molecules, the percentage of unique molecules, and the time taken to train the model.\n",
        "\n",
        "\n",
        "|     Network      | Train Loss on QM9  |   Valid   | Unique | Time (min) |\n",
        "| :--------------: | :----------------: | :-------: | :----: | :--------: |\n",
        "|      GTv1       |   0.0798(0.0067)   |   39.2%   | 100.0% |  11.3804   |\n",
        "| GTv2 (Weighted) |   0.0759(0.0023)   |   46.0%   | 99.8%  |  19.7241   |\n",
        "|  GTv2 (Gated)   |   0.0766(0.0026)   |   37.0%   | 100.0% |  22.0124   |\n",
        "|  GTv2 (Mixed)   | **0.0757**(0.0027) |   39.7%   | 100.0  |  20.6671   |\n",
        "|   GTv2 (FiLM)   |   0.0778(0.0034)   | **48.1%** | 100.0% |  23.1486   |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BFBd8sP5Ut2T"
      },
      "source": [
        "## References\n",
        "\n",
        "1. [Vijay Prakash Dwivedi and Xavier Bresson. \"A generalization of transformer networks to graphs.\" *arXiv preprint arXiv:2012.09699* (2020).](https://arxiv.org/abs/2012.09699)\n",
        "\n",
        "2. [Jonathan Ho, Ajay Jain and Pieter Abbeel. \"Denoising diffusion probabilistic models.\" Advances in neural information processing systems 33 (2020): 6840-6851.](https://proceedings.neurips.cc/paper/2020/hash/4c5bcfec8584af0d967f1ab10179ca4b-Abstract.html)\n",
        "\n",
        "3. [Ethan Perez, Florian Strub, Harm de Vries, Vincent Dumoulin and Aaron Courville. \"Film: Visual reasoning with a general conditioning layer.\" In Proceedings of the AAAI conference on artificial intelligence, vol. 32, no. 1. 2018.](https://ojs.aaai.org/index.php/AAAI/article/view/11671)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "v6bfywqcUt2T"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "gtv2",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.20"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
